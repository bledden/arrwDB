#!/usr/bin/env python3
"""
Quick Performance Flex - 30 Second Demo

Shows the most impressive capabilities in under 30 seconds:
- HNSW logarithmic scaling
- Sub-10ms search on 10k vectors
- Concurrent throughput

Run: python scripts/quick_flex.py
"""

import sys
import time
import numpy as np
from pathlib import Path
from uuid import uuid4
from concurrent.futures import ThreadPoolExecutor

sys.path.insert(0, str(Path(__file__).parent.parent))

from core.vector_store import VectorStore
from infrastructure.indexes.hnsw import HNSWIndex
from infrastructure.indexes.brute_force import BruteForceIndex


class C:
    """Colors"""
    G = '\033[92m'  # Green
    B = '\033[94m'  # Blue
    Y = '\033[93m'  # Yellow
    E = '\033[0m'   # End
    BOLD = '\033[1m'


def gen_vecs(n, d=128):
    """Generate normalized vectors"""
    vecs = np.random.randn(n, d).astype(np.float32)
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    return [vec / norms[i] for i, vec in enumerate(vecs)]


print(f"\n{C.BOLD}{C.B}{'='*70}{C.E}")
print(f"{C.BOLD}{C.B}  VECTOR DATABASE PERFORMANCE FLEX - 30 Second Demo{C.E}")
print(f"{C.BOLD}{C.B}{'='*70}{C.E}\n")

# Demo 1: Scaling comparison
print(f"{C.BOLD}1Ô∏è‚É£  Logarithmic Scaling Test{C.E}")
print("   Comparing HNSW vs Brute Force on different dataset sizes\n")

for n in [1000, 5000, 10000]:
    print(f"   {C.B}‚Üí {n:,} vectors:{C.E}", end=" ")

    # Brute Force
    vs_bf = VectorStore(dimension=128)
    bf = BruteForceIndex(vs_bf)
    vecs = gen_vecs(n)
    for v in vecs:
        cid = uuid4()
        idx = vs_bf.add_vector(cid, v)
        bf.add_vector(cid, idx)

    q = gen_vecs(1)[0]
    t = time.time()
    bf.search(q, k=10)
    bf_time = (time.time() - t) * 1000

    # HNSW
    vs_h = VectorStore(dimension=128)
    hnsw = HNSWIndex(vs_h, M=16, ef_construction=100, ef_search=50)
    for v in vecs:
        cid = uuid4()
        idx = vs_h.add_vector(cid, v)
        hnsw.add_vector(cid, idx)

    t = time.time()
    hnsw.search(q, k=10)
    hnsw_time = (time.time() - t) * 1000

    speedup = bf_time / hnsw_time
    print(f"BF {bf_time:.1f}ms | HNSW {hnsw_time:.1f}ms | {C.G}{speedup:.0f}x faster{C.E}")

print(f"\n{C.G}   ‚úì HNSW scales logarithmically (10x data ‚âà 2x slower){C.E}")

# Demo 2: Concurrent performance
print(f"\n{C.BOLD}2Ô∏è‚É£  Concurrent Search Performance{C.E}")
print("   Testing 20 simultaneous queries with Reader-Writer locks\n")

n = 5000
vs = VectorStore(dimension=128)
hnsw = HNSWIndex(vs, M=16, ef_construction=100, ef_search=50)

print(f"   Building index with {n:,} vectors...", end=" ", flush=True)
vecs = gen_vecs(n)
for v in vecs:
    cid = uuid4()
    idx = vs.add_vector(cid, v)
    hnsw.add_vector(cid, idx)
print("Done")

queries = gen_vecs(20)

# Sequential
print(f"   {C.B}Sequential (1 thread):{C.E}", end=" ", flush=True)
t = time.time()
for q in queries:
    hnsw.search(q, k=10)
seq_time = time.time() - t
print(f"{seq_time:.3f}s ({(seq_time/20)*1000:.1f}ms/query)")

# Concurrent
print(f"   {C.B}Concurrent (20 threads):{C.E}", end=" ", flush=True)
t = time.time()
with ThreadPoolExecutor(max_workers=20) as ex:
    list(ex.map(lambda q: hnsw.search(q, k=10), queries))
conc_time = time.time() - t
speedup = seq_time / conc_time
print(f"{conc_time:.3f}s ({C.G}{speedup:.1f}x speedup{C.E})")

print(f"\n{C.G}   ‚úì {20/conc_time:.0f} queries/second throughput{C.E}")

# Demo 3: The Flex
print(f"\n{C.BOLD}3Ô∏è‚É£  Production Scale Test{C.E}")
print("   Building and searching 20,000 high-dimensional vectors\n")

n = 20000
dimension = 512  # High-dimensional
vs = VectorStore(dimension=dimension)
hnsw = HNSWIndex(vs, M=16, ef_construction=200, ef_search=50)

print(f"   Building index ({n:,} x {dimension}D)...", end=" ", flush=True)
t_start = time.time()
vecs = gen_vecs(n, dimension)
for v in vecs:
    cid = uuid4()
    idx = vs.add_vector(cid, v)
    hnsw.add_vector(cid, idx)
build_time = time.time() - t_start
print(f"{build_time:.2f}s ({n/build_time:.0f} vec/s)")

# Search benchmark
queries = gen_vecs(50, dimension)
search_times = []

print(f"   Running 50 searches...", end=" ", flush=True)
for q in queries:
    t = time.time()
    hnsw.search(q, k=10)
    search_times.append((time.time() - t) * 1000)

avg = np.mean(search_times)
p95 = np.percentile(search_times, 95)
p99 = np.percentile(search_times, 99)
print(f"Done")

print(f"\n   {C.B}Search Latency:{C.E}")
print(f"   ‚Ä¢ Average: {avg:.2f}ms")
print(f"   ‚Ä¢ p95: {p95:.2f}ms")
print(f"   ‚Ä¢ p99: {p99:.2f}ms")

# Memory
stats = vs.get_statistics()
mem_mb = (stats['unique_vectors'] * dimension * 4) / (1024 * 1024)
print(f"\n   {C.B}Memory: {C.E}{mem_mb:.1f} MB for {n:,} {dimension}D vectors")

# THE FLEX
print(f"\n{C.BOLD}{C.G}{'üî• THE FLEX üî•'.center(70)}{C.E}")
print(f"{C.G}‚úì Searched 20,000 high-dimensional vectors in {avg:.1f}ms average{C.E}")
print(f"{C.G}‚úì Sub-{p95:.0f}ms p95 latency at scale{C.E}")
print(f"{C.G}‚úì {n/build_time:.0f} vectors/sec build throughput{C.E}")
print(f"{C.G}‚úì Production-ready with WAL, snapshots, thread safety{C.E}")
print(f"{C.G}‚úì Custom HNSW implementation (no external libraries){C.E}\n")

print(f"{C.BOLD}This is a from-scratch vector database with enterprise features.{C.E}")
print(f"{C.BOLD}Total demo time: {time.time() - t_start + seq_time + build_time:.1f}s{C.E}\n")
